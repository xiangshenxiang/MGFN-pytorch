# MGFN-pytorch
Here is the code for MGFN model, from the paper Multimodal Gate Fusion Network for Visual Question Answering. The code architecture is created according to the  [OpenVQA](https://github.com/MILVLG/openvqa) platform, where more detailed information can also be found.

## Citation
~~~
@misc{yu2019openvqa,
  author = {Yu, Zhou and Cui, Yuhao and Shao, Zhenwei and Gao, Pengbing and Yu, Jun},
  title = {OpenVQA},
  howpublished = {\url{https://github.com/MILVLG/openvqa}},
  year = {2019}
}
~~~

```
@inProceedings{yu2019mcan,
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  title = {Deep Modular Co-Attention Networks for Visual Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {6281--6290},
  year = {2019}
}
```

```
```

Installation, Data Preparation ,Training, and Evaluation
Please follow the README of  [MCAN](https://github.com/MILVLG/mcan-vqa) and [OpenVQA](https://github.com/MILVLG/openvqa).
